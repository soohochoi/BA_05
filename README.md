# BA_05 Semi-supervised Learning

<p align="center"><img width="604" alt="image" src="https://user-images.githubusercontent.com/97882448/208838979-8e8275d1-1cba-46e7-a584-da075e5a9c54.png">

ì´ìë£ŒëŠ” ê³ ë ¤ëŒ€í•™êµ ë¹„ë‹ˆì§€ìŠ¤ ì• ë„ë¦¬í‹±ìŠ¤ ê°•í•„ì„±êµìˆ˜ë‹˜ê»˜ ë°°ìš´ Semi-supervised learningì„ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.
ë¨¼ì €, Semi-supervised learningì˜ ë°©ì‹ì˜ ê¸°ë³¸ì ì¸ ê°œë…ì„ ë°°ìš´í›„ì— Consistency regularizationê°œë… ë° ì½”ë“œë¥¼ í†µí•´ ì§ì ‘ êµ¬í˜„ì„ í•´ë´„ìœ¼ë¡œì¨ ì´í•´ë¥¼ ë•ë„ë¡í•˜ê² ìŠµë‹ˆë‹¤.

## BA_05 Semi-supervised learning ê°œë…ì„¤ëª…

<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/209476685-f981bafd-fc68-40a5-9eb9-3460d7e6f185.png">

ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ í˜„ì‹¤ì—ì„œ ë°ì´í„°ì— ëŒ€í•œ ë‹µì´ ìˆëŠ” ê²½ìš°ë¥¼ ì˜ˆì¸¡í•  ë•Œ supervised learningì´ë¼ê³  í•˜ê³  ë‹µì´ ì—†ëŠ” ê²½ìš°ëŠ” unsupervised learningì´ë¼ê³  í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ê°€ ì‚´ì•„ê°€ëŠ” ì„¸ê³„ì—ëŠ” ë°ì´í„°ì˜ ë¼ë²¨ì´ ì¡°ê¸ˆë°–ì— ì—†ëŠ”ê²½ìš°ë„ ë‹¤ìˆ˜ë¡œ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ëŸ´ë•Œ ë¼ë²¨ì´ ì¡°ê¸ˆë§Œ ì—†ìœ¼ë©´ ê·¸ê²ƒì„ ì œì™¸ì‹œì¼œë„ ë˜ì§€ë§Œ ë‹¤ìˆ˜ì¸ê²½ìš° ë°ì´í„°ë¥¼ ë²„ë¦¬ê¸°ì—ëŠ” ë°ì´í„°ê°€ ì•„ê¹ìŠµë‹ˆë‹¤. ë˜í•œ ìœ„ì—ì„œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì„ ë³´ì‹œë©´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ë¶„ë¥˜ì„±ëŠ¥ì„ ë”ìš± ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ ëª¨ë“ ê²½ìš°ì— ê·¸ë ‡ì§€ëŠ” ì•Šê¸° ë•Œë¬¸ì— ë¶„ë¥˜ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œ Pseudo-labeling methodsì™€  Consistency regularization, Hybrid methodsì™€ ê°™ì€ ë°©ë²•ë¡ ë“¤ì´ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.

## BA_05 Semi-supervised learning-Consistency regularization
<p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/209476776-5b7927cf-4929-4ded-823f-9f4c1e252f21.png">

Consistency regularizationì´ë€ Unlabeled data pointì— ì‘ì€ ë³€í™”ë¥¼ ì£¼ì–´ë„ ì˜ˆì¸¡ì˜ ê²°ê³¼ì—ëŠ” ì¼ê´€ì„±ì´ ìˆì„ ê²ƒì´ë¼ëŠ” ê°€ì •ì—ì„œ ì¶œë°œí•©ë‹ˆë‹¤. ë˜í•œ Unlabeled dataëŠ” ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— data augmentationì„ í†µí•´ classê°€ ë°”ë€Œì§€ ì•Šì„ ì •ë„ì˜ ë³€í™”ë¥¼ ì¤¬ì„ ë•Œ, ì›ë˜ ë°ì´í„°ì™€ì˜ ì˜ˆì¸¡ê²°ê³¼ê°€ ê°™ì•„ì§€ë„ë¡ unsupervised lossë¥¼ ì£¼ì–´ í•™ìŠµí•©ë‹ˆë‹¤.

## Consistency regularization-Ladder network
  
<p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/209515640-c6b0c580-f9a8-4b7e-8dcd-1559ccb8e7d7.png">

Ladder networkë€ ì§€ë„í•™ìŠµê³¼ ë¹„ì§€ë„í•™ìŠµì„ ê²°í•©í•œ ë”¥ëŸ¬ë‹ ëª¨í˜•ì…ë‹ˆë‹¤.
ìœ„ìª½ê·¸ë¦¼ì„ ë³´ì‹œë©´ ë¹„ì§€ë„ í•™ìŠµ pre-trainingì—ì„œ ëë‚˜ì§€ ì•Šê³  ì§€ë„í•™ìŠµê³¼ í•¨ê»˜ trainingí•˜ë©° 2ê°œì˜ ì¸ì½”ë”ì™€ 1ê°œì˜ ë””ì½”ë”ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
Hierarchical latent variable modelì˜ ê³„ì¸µì ì¸ íŠ¹ì§•ì„ ë°˜ì˜í•œ ì˜¤í†  ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì˜€ê³  ì¸µë“¤ì˜ ì—°ê²°ì„ Deterministicì´ ì•„ë‹ˆë¼ Stochasticìœ¼ë¡œ ë°”ê¿” ì£¼ëŠ”ê²ƒì´ íŠ¹ì§•ì´ë©° íš¨ê³¼ì ì¸ í•™ìŠµì„ ìœ„í•´ Denoisingê¸°ë²•ì„ í™œìš©í•œ ëª¨ì–‘ì´ ê¼­ ì‚¬ë‹¤ë¦¬ì™€ ë‹®ì•„ ìˆì–´ Ladder networkë¼ê³  í•©ë‹ˆë‹¤. Denoisingì´ë€ ì¡ìŒì„ ì¶”ê°€í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” ë³¸ë˜ì˜ ê³ ìœ í•œ íŠ¹ì§•ì„ ë” ì˜ ì°¾ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. 
  
êµ¬ì¡°ë¥¼ ì¡°ê¸ˆë” ìì„¸íˆ ë³´ë©´ ë¨¼ì € ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ëŠ” ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ë„£ì€ corrupted  f pathì™€ Clean f pathì˜ ê°’ì´ ë¹¨ê°„ìƒ‰ì²˜ëŸ¼ ìœ ì‚¬í•˜ê²Œ í•™ìŠµì„í•©ë‹ˆë‹¤. ë˜í•œ corrupted f pathê°€ Denoising q pathë¡œ ë„˜ì–´ê°€ì„œ í•˜ëŠ˜ìƒ‰ì²˜ëŸ¼ Clean f pathì™€  hidden stateê°€ ìœ ì‚¬í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ë¼ë²¨ì´ ì—†ëŠ” ë°ì´í„°ëŠ” ë‹µì´ ì—†ê¸° ë•Œë¬¸ì— í•˜ëŠ˜ìƒ‰ê³¼ ê°™ì€ lossë§Œ ìµœì†Œí™” ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ê³  ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ë³€í˜•í•´ì„œ ë””ì½”ë”ì—ì„œ ê°€ì¥ ë†’ì€ layerë§Œ  ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì¸ ğ›¾âˆ’ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤

## Data set ì†Œê°œ
  ### Mnist
  
 <p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/209575433-8e15ddfb-2234-47b7-8276-64dee9e86ce8.png">

MNISTëŠ” ìˆ«ì 0ë¶€í„° 9ê¹Œì§€ì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ì†ê¸€ì”¨ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì˜ ê¸°ì›ì€ ê³¼ê±°ì— ìš°ì²´êµ­ì—ì„œ í¸ì§€ì˜ ìš°í¸ ë²ˆí˜¸ë¥¼ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œ ë§Œë“¤ì–´ì§„ í›ˆë ¨ ë°ì´í„°ì´ë©° ì´ 60,000ê°œì˜ í›ˆë ¨ ë°ì´í„°ì™€ ë ˆì´ë¸”, ì´ 10,000ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ë ˆì´ë¸”ë¡œ êµ¬ì„±ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ë ˆì´ë¸”ì€ 0ë¶€í„° 9ê¹Œì§€ ì´ 10ê°œì´ë©° ì´ ì˜ˆì œëŠ” ë¨¸ì‹  ëŸ¬ë‹ì„ ì²˜ìŒ ë°°ìš¸ ë•Œ ì ‘í•˜ê²Œ ë˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜ˆì œì´ê¸°ë„ í•©ë‹ˆë‹¤. 
  ### Fashoin_Mnist
 
  <p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/209575744-fd50e361-5fd8-468c-ac03-7cec14ecade2.png">

Fashoin_MnistëŠ” ê¸°ì¡´ì˜ MNIST ë°ì´í„°ì…‹(10ê°œ ì¹´í…Œê³ ë¦¬ì˜ ì†ìœ¼ë¡œ ì“´ ìˆ«ì)ì„ ëŒ€ì‹ í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…ë‹ˆë‹¤. MNISTì™€ ë™ì¼í•œ ì´ë¯¸ì§€ í¬ê¸°(28x28)ì´ë©° í›ˆë ¨ë°ì´í„°ì™€ í•™ìŠµë°ì´í„°ì˜ ìˆ˜ë„ ë™ì¼í•©ë‹ˆë‹¤. ëŒ€ì‹  ë¼ë²¨ì´ ìˆ«ìê°€ ì•„ë‹ˆë¼ ì˜·ì´ë©° 0ë¶€í„° 9ê¹Œì§€ ì´ë¦„ì€ 0:í‹°ì…”ì¸ /íƒ‘, 1: ë°”ì§€, 2: í’€ì˜¤ë²„(ìŠ¤ì›¨í„°ì˜ ì¼ì¢…) ,3: ë“œë ˆìŠ¤, 4: ì½”íŠ¸, 5: ìƒŒë“¤, 6: ì…”ì¸ , 7: ìŠ¤ë‹ˆì»¤ì¦ˆ, 8: ê°€ë°©, 9:ì•µí´ ë¶€ì¸  ì…ë‹ˆë‹¤.
    

## ì½”ë“œêµ¬í˜„-laddder_networks ëª¨ë¸ìƒì„±

```python
 #ëª¨ë¸ì„ ë§Œë“¤ê¸°ìœ„í•´ kerasë¥¼ ë¶ˆëŸ¬ì˜´
import keras
from keras.models import *
from keras.layers import *

import tensorflow as tf

#ê° ë ˆì´ì–´ë§ˆë‹¤ ë² íƒ€ê°’ì„ ìƒì„±í•´ì£¼ëŠ” í•¨ìˆ˜
class AddBeta(Layer):
    def __init__(self  , **kwargs):
        super(AddBeta, self).__init__(**kwargs)
        
    def build(self, input_shape):
        if self.built:
            return
        
        self.beta = self.add_weight(name='beta', 
                                      shape= input_shape[1:] ,
                                      initializer='zeros',
                                      trainable=True)
       
        self.built = True
        super(AddBeta, self).build(input_shape)  
        
    def call(self, x, training=None):
        return tf.add(x, self.beta)

#ì¸ì½”ë”©ì— ëŒ€í•œ ê°’ì„ ë””ì½”ë”©í•´ì£¼ê¸° ìœ„í•œ í•¨ìˆ˜
class G_Guass(Layer):
    def __init__(self , **kwargs):
        super(G_Guass, self).__init__(**kwargs)
        
    def wi(self, init, name):
        if init == 1:
            return self.add_weight(name='guess_'+name, 
                                      shape=(self.size,),
                                      initializer='ones',
                                      trainable=True)
        elif init == 0:
            return self.add_weight(name='guess_'+name, 
                                      shape=(self.size,),
                                      initializer='zeros',
                                      trainable=True)
        else:
            raise ValueError("Invalid argument '%d' provided for init in G_Gauss layer" % init)

#ë ˆì´ì–´ì— í›ˆë ¨ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    def build(self, input_shape):

        self.size = input_shape[0][-1]

        init_values = [0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]
        self.a = [self.wi(v, 'a' + str(i + 1)) for i, v in enumerate(init_values)]
        super(G_Guass , self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        z_c, u = x 

        def compute(y):
            return y[0] * tf.sigmoid(y[1] * u + y[2]) + y[3] * u + y[4]

        mu = compute(self.a[:5])
        v  = compute(self.a[5:])

        z_est = (z_c - mu) * v + mu
        return z_est
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0][0], self.size)

# ë°°ì¹˜ ì •ê·œí™” í•¨ìˆ˜
def batch_normalization(batch, mean=None, var=None):
    if mean is None or var is None:
        mean, var = tf.nn.moments(batch, axes=[0])
    return (batch - mean) / tf.sqrt(var + tf.constant(1e-10))

#ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_noise( inputs , noise_std ):
    return Lambda( lambda x: x + tf.random.normal(tf.shape(x)) * noise_std  )( inputs )

#ìœ„ì— ì •ì˜ëœ í•¨ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ladder ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±ê·¸ë¦¬ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
def get_ladder_network_fc(layer_sizes=[784, 1000, 500, 250, 250, 250, 10], 
     noise_std=0.3,
     denoising_cost=[1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]):
    #ë ˆì´ì–´ì˜ ê°¯ìˆ˜ ìƒì„±
    L = len(layer_sizes) - 1

    inputs_l = Input((layer_sizes[0],))  
    inputs_u = Input((layer_sizes[0],))  

    fc_enc = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[1:] ]
    fc_dec = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[:-1]]
    betas  = [AddBeta() for l in range(L)]
    #ì¸ì½”ë” ì •ì˜
    def encoder(inputs, noise_std  ):
        h = add_noise(inputs, noise_std)
        all_z    = [None for _ in range( len(layer_sizes))]
        all_z[0] = h
        
        for l in range(1, L+1):
            z_pre = fc_enc[l-1](h)
            z =     Lambda(batch_normalization)(z_pre) 
            z =     add_noise (z, noise_std)
            
            if l == L:
                h = Activation('softmax')(betas[l-1](z))
            else:
                h = Activation('relu')(betas[l-1](z))
                
            all_z[l] = z

        return h, all_z

    y_c_l, _ = encoder(inputs_l, noise_std)
    y_l, _   = encoder(inputs_l, 0.0)  

    y_c_u, corr_z  = encoder(inputs_u , noise_std)
    y_u,  clean_z  = encoder(inputs_u , 0.0)  

    # ë””ì½”ë” ì •ì˜ ë°  ì½”ìŠ¤íŠ¸ ì €ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    d_cost = []  # to store the denoising cost of all layers
    for l in range(L, -1, -1):
        z, z_c = clean_z[l], corr_z[l]
        if l == L:
            u = y_c_u
        else:
            u = fc_dec[l]( z_est ) 
        u = Lambda(batch_normalization)(u)
        z_est  = G_Guass()([z_c, u])  
        d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est - z), 1)) / layer_sizes[l]) * denoising_cost[l])

    u_cost = tf.add_n(d_cost)

    y_c_l = Lambda(lambda x: x[0])([y_c_l, y_l, y_c_u, y_u, u, z_est, z])

    tr_m = Model([inputs_l, inputs_u], y_c_l)
    tr_m.add_loss(u_cost)
    # adam í™œìš©í•´ ìµœì í™” ì‹œí‚¤ê³ 
    tr_m.compile(keras.optimizers.Adam(lr=0.02 ), 'categorical_crossentropy', metrics=['accuracy'])
    #dense lossì¶”ì¶œí•˜ë„ë¡ í•¨
    tr_m.metrics_names.append("final_loss")
    tr_m.metrics_tensors.append(u_cost)
    te_m = Model(inputs_l, y_l)
    tr_m.test_model = te_m

    return tr_m
 ```
## ì½”ë“œêµ¬í˜„-ì‹¤í–‰íŒŒì¼
 ```python
#from keras.datasets import mnist
from tensorflow.keras.datasets import fashion_mnist
import keras
import random
from sklearn.metrics import accuracy_score
import numpy as np
from ladder_networks import ladder_network

# mnist ë°ì´í„°ì˜ ì‚¬ì´ì¦ˆ
data_size = 28*28
#yê°’ì˜ ê°¯ìˆ˜
labels = 10
#ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜´
(x_train, y_train), (x_test, y_test) =  fashion_mnist.load_data()
#28*28ì„ 1*784ë¡œ ë§Œë“¤ì–´ì£¼ê³  255ë¥¼ ë‚˜ëˆ ì„œ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê¸°ìœ„í•œ ë°©ë²•
x_train = x_train.reshape(60000, data_size).astype('float32')/255
x_test  = x_test.reshape(10000,  data_size).astype('float32')/255
#ì¹´íƒœê³ ë¦¬ì»¬ í•˜ê²Œ ë¼ë²¨ë“¤ì„ ë°”ê¾¸ì–´ì¤Œ one-hot ì¸ì½”ë”©ì„
y_train = keras.utils.to_categorical(y_train, labels)
y_test  = keras.utils.to_categorical(y_test,  labels)

# 100ê°œ ë°ì´í„° ëœë¤ì¶”ì¶œ
numbers_shape = range(x_train.shape[0])
random.seed(2023)
numbers_shape = np.random.choice(x_train.shape[0], 100)

Unlabeled_x_train = x_train
labeled_x_train = x_train[numbers_shape]
labeled_y_train = y_train[numbers_shape]
#ëª«ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ìŒ concatí•´ì„œ traindata ê°¯ìˆ˜ ë§ì¶°ì£¼ê¸°
quont = Unlabeled_x_train.shape[0] // labeled_x_train.shape[0]
labeled_x_train_quont = np.concatenate([labeled_x_train]*quont)
labeled_y_train_quont = np.concatenate([labeled_y_train]*quont)

# ëª¨ë¸ìƒì„±
model = get_ladder_network_fc(layer_sizes=[data_size, 1000, 500, 250, 250, 250, labels])

# ë°°ì¹˜ì‚¬ì´ì¦ˆë¥¼ 100ë²ˆ ëŒë¦¼
for _ in range(100):
    model.fit([labeled_x_train_quont, Unlabeled_x_train], labeled_y_train_quont, epochs=100)
    y_test_pr = model.test_model.predict(x_test, batch_size=100)
    print("Test ì •í™•ë„ëŠ” %f ë‚˜ì™”ìŠµë‹ˆë‹¤." % accuracy_score(y_test.argmax(-1), y_test_pr.argmax(-1)))
 ```
##ê²°ë¡ 
 
